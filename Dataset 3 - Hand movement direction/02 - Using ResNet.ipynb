{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residual Networks are pretty simple actually. Sounds complex but its really not all that special, except in the results they produce. \n",
    "\n",
    "- Think of a normal network where you have 5 layers, A to E. \n",
    "- In a residual network, the only difference is that output of A not only goes into B, but also goes into layer C. This is called a **Skip Connection.**\n",
    "- People who've done Control Systems can think of it as a feedforward loop. Funnily enough, it has a similar functionality of optimizing.\n",
    "- A skip connection can also have additional layers before being added into the *'forward'* path.\n",
    "- Here's a simplified diagram.\n",
    "\n",
    "![image](img/resnet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But Why ResNets?\n",
    "\n",
    "The idea behind ResNets is pretty much, \"Hey, it doesn't hurt. So why not?\" <br>\n",
    "Surprisingly, this ended up being pretty big because it *helped* a lot more than it hurt.\n",
    "\n",
    "- Residual Networks have a unique propert of being able to **\"learn\" the Identity Function**\n",
    "- In layman terms, considering the worst case scenario, a residual connection will give you an output that's exactly the same as the input. i.e <br>**Input x Residual Network = Input**      <br>\n",
    "(Identity Function I follows *A x I = A*)\n",
    "- So these extra layers don't affect training error negatively, it doesn't \"hurt\" and this is in the absolute worst cases. At most, it just increases time and computational cost.\n",
    "- In the best cases however, each layer learns something based on past and current inputs, and has the advantage of not digressing down a path that leads to very high error. \n",
    "- Normal Networks (with no skipped connections) find it harder and harder to choose and learn correctly the deeper the network gets. Simply put, more layers of a normal network leads to higher chances of error\n",
    "- However, since a Residual Network continuously checks and adds the past input, the training error doesn't ever grow exponentially!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So,  let's do this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "_______\n",
    "## Imports and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataPreprocess import bandwise_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataPreProcess is the file with all the preprocessing and splitting into frequency bands from the previous notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = r'C:\\Users\\Aditi\\Documents\\Machine Learning\\BCI Dataset\\BCICIV_3_mat\\S1.mat'\n",
    "file2 = r'C:\\Users\\Aditi\\Documents\\Machine Learning\\BCI Dataset\\BCICIV_3_mat\\S2.mat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape - True\n",
      "y sum - True\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = bandwise_split(file2, split = True, train_size=0.8) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# samples, time, channels\n",
      "(1280, 400, 5)\n",
      "(1280, 4)\n",
      "(320, 400, 5)\n",
      "(320, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"# samples, time, channels\")\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______\n",
    "So we have our data ready, time to make our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\aditi\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:72: UserWarning: h5py is running against HDF5 1.10.2 when it was built against 1.10.3, this may cause problems\n",
      "  '{0}.{1}.{2}'.format(*version.hdf5_built_version_tuple)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import (Input, Conv1D, MaxPooling1D, Dropout,\n",
    "                          BatchNormalization, Activation, Add,\n",
    "                          Flatten, Dense)\n",
    "from keras.models import Model, load_model\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import (ModelCheckpoint,\n",
    "                             TensorBoard, ReduceLROnPlateau,\n",
    "                             CSVLogger, EarlyStopping)\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from ResNet import ResidualUnit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Residual unit codes are from [here](https://github.com/antonior92/automatic-ecg-diagnosis) who made an Automatic ECG analyser. They used a convolutional neural network even though its not an image, and it brought very good results. So I tried the same with this EEG dataset, and yepp, definitely gave good results.<br>\n",
    "After a spectacular failed trial with RNNs, I found this usage of a residual network, and it proved fruitful!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\aditi\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\aditi\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\aditi\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\aditi\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\aditi\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\aditi\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    }
   ],
   "source": [
    "kernel_size = 8\n",
    "kernel_initializer = 'he_normal'\n",
    "signal = Input(shape=(400, 5), dtype=np.float32, name='signal')\n",
    "\n",
    "x = signal\n",
    "x = Conv1D(32, kernel_size, padding='same', use_bias=False,\n",
    "           kernel_initializer=kernel_initializer)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x, y = ResidualUnit(200, 64, kernel_size=kernel_size,\n",
    "                    kernel_initializer=kernel_initializer)([x, x])\n",
    "x, y = ResidualUnit(100, 128, kernel_size=kernel_size,\n",
    "                    kernel_initializer=kernel_initializer)([x, y])\n",
    "x, y = ResidualUnit(50, 256, kernel_size=kernel_size,\n",
    "                    kernel_initializer=kernel_initializer)([x, y])\n",
    "x, y = ResidualUnit(25, 512, kernel_size=kernel_size,\n",
    "                    kernel_initializer=kernel_initializer)([x, y])\n",
    "x, _ = ResidualUnit(12, 512, kernel_size=5,\n",
    "                    kernel_initializer=kernel_initializer)([x, y])\n",
    "\n",
    "x = Flatten()(x)\n",
    "diagn = Dense(4, activation='sigmoid', kernel_initializer=kernel_initializer)(x)\n",
    "model = Model(signal, diagn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Network Architecture\n",
    "So here we have 5 Residual Blocks. <br>Each Residual Block has 2 Conv layers in the forward path and ne mini Conv layer in the skipped connection path. Add the Batch Norm and Pooling layers and you have well, a lot of layers. <br>\n",
    "\n",
    "At the end of every Convolutional Net, there's always a Dense layer, or a fully connected layer. This is the actual \"classifier layer\" that will calculate which class the input belongs too. <br>(Maybe I should add another Dense layer? )\n",
    "____\n",
    "\n",
    "This is what the architecture of one Residual Block looks like (the dimensions correspond to the first block)\n",
    "![image](img/resblock.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: **Batch Normalization (BN) - what is it?** <br>\n",
    "- In general, the convention for preprocessing one's input data is to normalize the data right?\n",
    "- You try to make sure all the features lie in the same range so that no one feature gets priority becuase the original values are simply higher.\n",
    "- That's what Batch Norm layers do as well, but for *the Neural Network co-efficients.*\n",
    "- These layers make sure that none of the weights that you are optimizing gets too large or too small , and for this, you normalize the inputs to the layers routinely throughout your network.\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note2: **Pooling, why?**\n",
    "\n",
    "The \"Max-Pooling Layer\" is simple a maximum kernel (for all you Image Processing people, this will sound familiar) \n",
    "It takes the max of every n-neighbour cells along the horizontal and vertical axes throughout the picture. \n",
    "- Pooling layers have one functionality - **highlight the most important features.**\n",
    "- You reduce the parameters after each convolution, and you choose the reduced parameters such that only the \"best\" or \"most important\" ones remain. Hence, the max function.\n",
    "- I've heard average pooling is done too, but Max-pooling has worked efficiently in numerous networks before, so it has an empirical basis for being the best choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "signal (InputLayer)             (None, 400, 5)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 400, 32)      1280        signal[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 400, 32)      128         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 400, 32)      0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 400, 64)      16384       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 400, 64)      256         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 400, 64)      0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 400, 64)      0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 200, 32)      0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 200, 64)      32768       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 200, 64)      2048        max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 200, 64)      0           conv1d_4[0][0]                   \n",
      "                                                                 conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 200, 64)      256         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 200, 64)      0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 200, 64)      0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 200, 128)     65536       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 200, 128)     512         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 200, 128)     0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 200, 128)     0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 100, 64)      0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 100, 128)     131072      dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 100, 128)     8192        max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 100, 128)     0           conv1d_7[0][0]                   \n",
      "                                                                 conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 100, 128)     512         add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 100, 128)     0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 100, 128)     0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 100, 256)     262144      dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 100, 256)     1024        conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 100, 256)     0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 100, 256)     0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 50, 128)      0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 50, 256)      524288      dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 50, 256)      32768       max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 50, 256)      0           conv1d_10[0][0]                  \n",
      "                                                                 conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 50, 256)      1024        add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 50, 256)      0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 50, 256)      0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 50, 512)      1048576     dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 50, 512)      2048        conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 50, 512)      0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 50, 512)      0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 25, 256)      0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 25, 512)      2097152     dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 25, 512)      131072      max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 25, 512)      0           conv1d_13[0][0]                  \n",
      "                                                                 conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 25, 512)      2048        add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 25, 512)      0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 25, 512)      0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 25, 512)      1310720     dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 25, 512)      2048        conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 25, 512)      0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 25, 512)      0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 13, 512)      1310720     dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 13, 512)      0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 13, 512)      0           conv1d_15[0][0]                  \n",
      "                                                                 max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 13, 512)      2048        add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 13, 512)      0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 13, 512)      0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 6656)         0           dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 4)            26628       flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 7,013,252\n",
      "Trainable params: 7,007,300\n",
      "Non-trainable params: 5,952\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\aditi\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\aditi\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\aditi\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "loss = 'binary_crossentropy'\n",
    "lr = 0.001\n",
    "batch_size = 64\n",
    "opt = Adam(lr)\n",
    "callbacks = [ReduceLROnPlateau(monitor='val_loss',\n",
    "                               factor=0.1,\n",
    "                               patience=7,\n",
    "                               min_lr=lr / 10),\n",
    "             EarlyStopping(patience=9, min_delta=0.00001)]\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))\n",
    "model.compile(loss=loss, optimizer=opt, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are some complicated additions here. <br>\n",
    "- The ReduceLROnPlateau (wow tat's a mouthful) is a function that reduces the learning rate once the validation loss starts to become constant.\n",
    "- This is to ensure that we don't skip the optima with larger Larning rate values that are needed at the start of training.\n",
    "- EarlyStopping is to ensure that we don't skip the optimum and move back up the convex during gradient descent. \n",
    "- In simpler terms, these are to ensure that training error stays minimum and that the optimum value is reached with the best accuracy\n",
    "_________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To create logs\n",
    "callbacks += [TensorBoard(log_dir='./logs', batch_size=batch_size, write_graph=False),\n",
    "              CSVLogger('training.log', append=False)]  # Change append to true if continuing training\n",
    "# Save the BEST and LAST model\n",
    "callbacks += [ModelCheckpoint('./backup_model_last.hdf5'),\n",
    "              ModelCheckpoint('./backup_model_best.hdf5', save_best_only=True)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "## Training\n",
    "**Now that we have made our model and added all the required parameters, let's train!** <br> (I know I've put 200 epochs, but it rarely ever completes 200 fully because the optimum value is reached before hand, generally around 57 - 90, but this is just a safeguard)<br> (Also, a lot of tensorflow warnings come up regarding Dropout rate and stuff, just ignore them, don't worry about it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\aditi\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\aditi\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 1216 samples, validate on 64 samples\n",
      "WARNING:tensorflow:From c:\\users\\aditi\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\aditi\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\aditi\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\aditi\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\aditi\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages\\keras\\callbacks.py:850: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\aditi\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages\\keras\\callbacks.py:856: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "Epoch 1/200\n",
      "1216/1216 [==============================] - 44s 36ms/step - loss: 0.9525 - acc: 0.6587 - val_loss: 0.6295 - val_acc: 0.7500\n",
      "WARNING:tensorflow:From c:\\users\\aditi\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages\\keras\\callbacks.py:995: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n",
      "Epoch 2/200\n",
      "1216/1216 [==============================] - 44s 36ms/step - loss: 0.8552 - acc: 0.6752 - val_loss: 0.6432 - val_acc: 0.7148\n",
      "Epoch 3/200\n",
      "1216/1216 [==============================] - 45s 37ms/step - loss: 0.7995 - acc: 0.6891 - val_loss: 0.6094 - val_acc: 0.7383\n",
      "Epoch 4/200\n",
      "1216/1216 [==============================] - 47s 39ms/step - loss: 0.7377 - acc: 0.6994 - val_loss: 0.5484 - val_acc: 0.7500\n",
      "Epoch 5/200\n",
      "1216/1216 [==============================] - 47s 39ms/step - loss: 0.7253 - acc: 0.7021 - val_loss: 0.5948 - val_acc: 0.7500\n",
      "Epoch 6/200\n",
      "1216/1216 [==============================] - 48s 39ms/step - loss: 0.7162 - acc: 0.7052 - val_loss: 0.6067 - val_acc: 0.7539\n",
      "Epoch 7/200\n",
      "1216/1216 [==============================] - 49s 40ms/step - loss: 0.6829 - acc: 0.7144 - val_loss: 0.5542 - val_acc: 0.7461\n",
      "Epoch 8/200\n",
      "1216/1216 [==============================] - 48s 39ms/step - loss: 0.6539 - acc: 0.7231 - val_loss: 0.5309 - val_acc: 0.7617\n",
      "Epoch 9/200\n",
      "1216/1216 [==============================] - 48s 39ms/step - loss: 0.6626 - acc: 0.7272 - val_loss: 0.5123 - val_acc: 0.7773\n",
      "Epoch 10/200\n",
      "1216/1216 [==============================] - 48s 39ms/step - loss: 0.6423 - acc: 0.7307 - val_loss: 0.5076 - val_acc: 0.7812\n",
      "Epoch 11/200\n",
      "1216/1216 [==============================] - 47s 39ms/step - loss: 0.6320 - acc: 0.7313 - val_loss: 0.4863 - val_acc: 0.7539\n",
      "Epoch 12/200\n",
      "1216/1216 [==============================] - 50s 41ms/step - loss: 0.6097 - acc: 0.7399 - val_loss: 0.4801 - val_acc: 0.7578\n",
      "Epoch 13/200\n",
      "1216/1216 [==============================] - 47s 38ms/step - loss: 0.6111 - acc: 0.7301 - val_loss: 0.4883 - val_acc: 0.7617\n",
      "Epoch 14/200\n",
      "1216/1216 [==============================] - 47s 39ms/step - loss: 0.5798 - acc: 0.7477 - val_loss: 0.4721 - val_acc: 0.7773\n",
      "Epoch 15/200\n",
      "1216/1216 [==============================] - 47s 38ms/step - loss: 0.5544 - acc: 0.7510 - val_loss: 0.4944 - val_acc: 0.7422\n",
      "Epoch 16/200\n",
      "1216/1216 [==============================] - 47s 38ms/step - loss: 0.5504 - acc: 0.7531 - val_loss: 0.4876 - val_acc: 0.7539\n",
      "Epoch 17/200\n",
      "1216/1216 [==============================] - 46s 38ms/step - loss: 0.5355 - acc: 0.7568 - val_loss: 0.4894 - val_acc: 0.7773\n",
      "Epoch 18/200\n",
      "1216/1216 [==============================] - 47s 38ms/step - loss: 0.5326 - acc: 0.7593 - val_loss: 0.4665 - val_acc: 0.7695\n",
      "Epoch 19/200\n",
      "1216/1216 [==============================] - 46s 38ms/step - loss: 0.5200 - acc: 0.7662 - val_loss: 0.4796 - val_acc: 0.7852\n",
      "Epoch 20/200\n",
      "1216/1216 [==============================] - 46s 38ms/step - loss: 0.4950 - acc: 0.7827 - val_loss: 0.4738 - val_acc: 0.8086\n",
      "Epoch 21/200\n",
      "1216/1216 [==============================] - 47s 38ms/step - loss: 0.4968 - acc: 0.7792 - val_loss: 0.4804 - val_acc: 0.7930\n",
      "Epoch 22/200\n",
      "1216/1216 [==============================] - 47s 39ms/step - loss: 0.4861 - acc: 0.7819 - val_loss: 0.5114 - val_acc: 0.7930\n",
      "Epoch 23/200\n",
      "1216/1216 [==============================] - 46s 38ms/step - loss: 0.4842 - acc: 0.7837 - val_loss: 0.4742 - val_acc: 0.8047\n",
      "Epoch 24/200\n",
      "1216/1216 [==============================] - 47s 39ms/step - loss: 0.4694 - acc: 0.7905 - val_loss: 0.4571 - val_acc: 0.7891\n",
      "Epoch 25/200\n",
      "1216/1216 [==============================] - 47s 38ms/step - loss: 0.4577 - acc: 0.7934 - val_loss: 0.4417 - val_acc: 0.8086\n",
      "Epoch 26/200\n",
      "1216/1216 [==============================] - 47s 39ms/step - loss: 0.4472 - acc: 0.7981 - val_loss: 0.4356 - val_acc: 0.7969\n",
      "Epoch 27/200\n",
      "1216/1216 [==============================] - 46s 38ms/step - loss: 0.4650 - acc: 0.7913 - val_loss: 0.4383 - val_acc: 0.7852\n",
      "Epoch 28/200\n",
      "1216/1216 [==============================] - 45s 37ms/step - loss: 0.4455 - acc: 0.8035 - val_loss: 0.4435 - val_acc: 0.7891\n",
      "Epoch 29/200\n",
      "1216/1216 [==============================] - 45s 37ms/step - loss: 0.4313 - acc: 0.8106 - val_loss: 0.4419 - val_acc: 0.7969\n",
      "Epoch 30/200\n",
      "1216/1216 [==============================] - 45s 37ms/step - loss: 0.4069 - acc: 0.8222 - val_loss: 0.3893 - val_acc: 0.8125\n",
      "Epoch 31/200\n",
      "1216/1216 [==============================] - 45s 37ms/step - loss: 0.4094 - acc: 0.8209 - val_loss: 0.4105 - val_acc: 0.8047\n",
      "Epoch 32/200\n",
      "1216/1216 [==============================] - 46s 37ms/step - loss: 0.4259 - acc: 0.8129 - val_loss: 0.3870 - val_acc: 0.8125\n",
      "Epoch 33/200\n",
      "1216/1216 [==============================] - 45s 37ms/step - loss: 0.3873 - acc: 0.8306 - val_loss: 0.3848 - val_acc: 0.8164\n",
      "Epoch 34/200\n",
      "1216/1216 [==============================] - 45s 37ms/step - loss: 0.3886 - acc: 0.8215 - val_loss: 0.3747 - val_acc: 0.8281\n",
      "Epoch 35/200\n",
      "1216/1216 [==============================] - 45s 37ms/step - loss: 0.3685 - acc: 0.8357 - val_loss: 0.3681 - val_acc: 0.8516\n",
      "Epoch 36/200\n",
      "1216/1216 [==============================] - 45s 37ms/step - loss: 0.3660 - acc: 0.8427 - val_loss: 0.3826 - val_acc: 0.8242\n",
      "Epoch 37/200\n",
      "1216/1216 [==============================] - 45s 37ms/step - loss: 0.3536 - acc: 0.8466 - val_loss: 0.3598 - val_acc: 0.8555\n",
      "Epoch 38/200\n",
      "1216/1216 [==============================] - 45s 37ms/step - loss: 0.3642 - acc: 0.8448 - val_loss: 0.3273 - val_acc: 0.8516\n",
      "Epoch 39/200\n",
      "1216/1216 [==============================] - 46s 38ms/step - loss: 0.3499 - acc: 0.8514 - val_loss: 0.3519 - val_acc: 0.8477\n",
      "Epoch 40/200\n",
      "1216/1216 [==============================] - 47s 38ms/step - loss: 0.3537 - acc: 0.8479 - val_loss: 0.3931 - val_acc: 0.8359\n",
      "Epoch 41/200\n",
      "1216/1216 [==============================] - 45s 37ms/step - loss: 0.3283 - acc: 0.8546 - val_loss: 0.3792 - val_acc: 0.8516\n",
      "Epoch 42/200\n",
      "1216/1216 [==============================] - 47s 39ms/step - loss: 0.3125 - acc: 0.8651 - val_loss: 0.3188 - val_acc: 0.8594\n",
      "Epoch 43/200\n",
      "1216/1216 [==============================] - 45s 37ms/step - loss: 0.3204 - acc: 0.8596 - val_loss: 0.2890 - val_acc: 0.8789\n",
      "Epoch 44/200\n",
      "1216/1216 [==============================] - 46s 37ms/step - loss: 0.3164 - acc: 0.8676 - val_loss: 0.3008 - val_acc: 0.8750\n",
      "Epoch 45/200\n",
      "1216/1216 [==============================] - 46s 38ms/step - loss: 0.3161 - acc: 0.8620 - val_loss: 0.2677 - val_acc: 0.8750\n",
      "Epoch 46/200\n",
      "1216/1216 [==============================] - 46s 38ms/step - loss: 0.2991 - acc: 0.8699 - val_loss: 0.3033 - val_acc: 0.8750\n",
      "Epoch 47/200\n",
      "1216/1216 [==============================] - 46s 38ms/step - loss: 0.2930 - acc: 0.8760 - val_loss: 0.2987 - val_acc: 0.8828\n",
      "Epoch 48/200\n",
      "1216/1216 [==============================] - 46s 37ms/step - loss: 0.2827 - acc: 0.8808 - val_loss: 0.2713 - val_acc: 0.8945\n",
      "Epoch 49/200\n",
      "1216/1216 [==============================] - 46s 37ms/step - loss: 0.2928 - acc: 0.8758 - val_loss: 0.2613 - val_acc: 0.8945\n",
      "Epoch 50/200\n",
      "1216/1216 [==============================] - 46s 38ms/step - loss: 0.2751 - acc: 0.8869 - val_loss: 0.2740 - val_acc: 0.8789\n",
      "Epoch 51/200\n",
      "1216/1216 [==============================] - 46s 38ms/step - loss: 0.2767 - acc: 0.8789 - val_loss: 0.2589 - val_acc: 0.8984\n",
      "Epoch 52/200\n",
      "1216/1216 [==============================] - 45s 37ms/step - loss: 0.2522 - acc: 0.8939 - val_loss: 0.2359 - val_acc: 0.9062\n",
      "Epoch 53/200\n",
      "1216/1216 [==============================] - 45s 37ms/step - loss: 0.2603 - acc: 0.8917 - val_loss: 0.2396 - val_acc: 0.9023\n",
      "Epoch 54/200\n",
      "1216/1216 [==============================] - 46s 38ms/step - loss: 0.2498 - acc: 0.8914 - val_loss: 0.2478 - val_acc: 0.8828\n",
      "Epoch 55/200\n",
      "1216/1216 [==============================] - 46s 38ms/step - loss: 0.2525 - acc: 0.8886 - val_loss: 0.2590 - val_acc: 0.8906\n",
      "Epoch 56/200\n",
      "1216/1216 [==============================] - 45s 37ms/step - loss: 0.2436 - acc: 0.8929 - val_loss: 0.2412 - val_acc: 0.9062\n",
      "Epoch 57/200\n",
      "1216/1216 [==============================] - 56s 46ms/step - loss: 0.2428 - acc: 0.8958 - val_loss: 0.2439 - val_acc: 0.8906\n",
      "Epoch 58/200\n",
      "1216/1216 [==============================] - 49s 41ms/step - loss: 0.2243 - acc: 0.9069 - val_loss: 0.2369 - val_acc: 0.9102\n",
      "Epoch 59/200\n",
      "1216/1216 [==============================] - 47s 39ms/step - loss: 0.2148 - acc: 0.9100 - val_loss: 0.2200 - val_acc: 0.9180\n",
      "Epoch 60/200\n",
      "1216/1216 [==============================] - 47s 38ms/step - loss: 0.2278 - acc: 0.9056 - val_loss: 0.2319 - val_acc: 0.8945\n",
      "Epoch 61/200\n",
      "1216/1216 [==============================] - 47s 39ms/step - loss: 0.2227 - acc: 0.9095 - val_loss: 0.2395 - val_acc: 0.9062\n",
      "Epoch 62/200\n",
      "1216/1216 [==============================] - 47s 39ms/step - loss: 0.2086 - acc: 0.9165 - val_loss: 0.2307 - val_acc: 0.9141\n",
      "Epoch 63/200\n",
      "1216/1216 [==============================] - 47s 38ms/step - loss: 0.2108 - acc: 0.9106 - val_loss: 0.2180 - val_acc: 0.9102\n",
      "Epoch 64/200\n",
      "1216/1216 [==============================] - 46s 38ms/step - loss: 0.2158 - acc: 0.9108 - val_loss: 0.1992 - val_acc: 0.9219\n",
      "Epoch 65/200\n",
      "1216/1216 [==============================] - 47s 39ms/step - loss: 0.2027 - acc: 0.9155 - val_loss: 0.2158 - val_acc: 0.9102\n",
      "Epoch 66/200\n",
      "1216/1216 [==============================] - 47s 38ms/step - loss: 0.1900 - acc: 0.9198 - val_loss: 0.1977 - val_acc: 0.9297\n",
      "Epoch 67/200\n",
      "1216/1216 [==============================] - 48s 39ms/step - loss: 0.1904 - acc: 0.9239 - val_loss: 0.1991 - val_acc: 0.9062\n",
      "Epoch 68/200\n",
      "1216/1216 [==============================] - 48s 40ms/step - loss: 0.1910 - acc: 0.9192 - val_loss: 0.1828 - val_acc: 0.9258\n",
      "Epoch 69/200\n",
      "1216/1216 [==============================] - 46s 38ms/step - loss: 0.1867 - acc: 0.9219 - val_loss: 0.1846 - val_acc: 0.9375\n",
      "Epoch 70/200\n",
      "1216/1216 [==============================] - 48s 40ms/step - loss: 0.1730 - acc: 0.9274 - val_loss: 0.1969 - val_acc: 0.9219\n",
      "Epoch 71/200\n",
      "1216/1216 [==============================] - 47s 39ms/step - loss: 0.1821 - acc: 0.9258 - val_loss: 0.2162 - val_acc: 0.9023\n",
      "Epoch 72/200\n",
      "1216/1216 [==============================] - 48s 39ms/step - loss: 0.1695 - acc: 0.9328 - val_loss: 0.1854 - val_acc: 0.9141\n",
      "Epoch 73/200\n",
      "1216/1216 [==============================] - 50s 41ms/step - loss: 0.1747 - acc: 0.9305 - val_loss: 0.1830 - val_acc: 0.9180\n",
      "Epoch 74/200\n",
      "1216/1216 [==============================] - 50s 41ms/step - loss: 0.1645 - acc: 0.9315 - val_loss: 0.1918 - val_acc: 0.9258\n",
      "Epoch 75/200\n",
      "1216/1216 [==============================] - 49s 40ms/step - loss: 0.1725 - acc: 0.9311 - val_loss: 0.1894 - val_acc: 0.9141\n",
      "Epoch 76/200\n",
      "1216/1216 [==============================] - 50s 41ms/step - loss: 0.1619 - acc: 0.9319 - val_loss: 0.1886 - val_acc: 0.9219\n",
      "Epoch 77/200\n",
      "1216/1216 [==============================] - 48s 40ms/step - loss: 0.1614 - acc: 0.9322 - val_loss: 0.1864 - val_acc: 0.9180\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=200,\n",
    "                    validation_split=0.05,\n",
    "                    shuffle='batch',\n",
    "                    callbacks=callbacks,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the hyperparameters\n",
    "**So, I tried a lot of different variations of the parameters below.** <br>\n",
    "1. The initializer, he_normal worked the best. He_uniform also works pretty decently<br>\n",
    "I tried changing the number of filters in the first layer to 16 and 8, but that seemed to be counter productive and reduced the final accuracy by a lot, and introduced a lot of variance surprisingly. I thought the jump from 400x5 to 400x32 is too much, but turns out it's just right.\n",
    "\n",
    "2. Tuning the number of layers, well, yikes it takes way too long to get the most optimized. <br> \n",
    "I started with 4 ResNet Units with different values of pooling and filters, these are optimum. Feel free to change the number of filters (32/64/128/256 to higher or lower) and test different combinations out. I plan on crating a table and seeing how much the accuracy improves or worsens and optimizing it. But my dear laptop is too slow for my impatience, so I'll get to that later (or run it on collab I guess)\n",
    "\n",
    "3. I didn't really change the activation functions much, relu (with classifier as sigmoid) have proven in MANY, MANY projects to be the best choice.\n",
    "\n",
    "4. Lastly, I tried tuning alpha. I wanted to reduce the learning rate when it seemed to be getting close to the global optimum. Hence the reduce LR on plateau. I tried a few different starting values, but 0.001 just seems to work really. Additionally, i tried an alpha decay, but they either decayed too fast or too slow. I'll see if I can include an exponential decay into the plateud LR and see if it helps. The sudden drop (halving of LR on plateau of val_loss) is counter-intuitive, but seems to work for many people so it can stay for now.\n",
    "\n",
    "____________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc', 'lr'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./final_model.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use this to load previous saved model\n",
    "model = load_model(\"./final_model.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "### Accuracy and related Metrics\n",
    "First we have to use the model to predict results for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320/320 [==============================] - 1s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "y_score = model.predict(X_test, batch_size=64, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I made this function before I realised sklearn and numpy have a relu function, sue me\n",
    "def relu(a):\n",
    "    b = []\n",
    "    for i in a:\n",
    "        ab = i==max(i)\n",
    "        b.append(ab*1.0)\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = relu(y_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an absolute accuracy, I used the following lines of code. AUC/ROC curves ended up giving me higher than the actual correct values. I may have screwed up a bit there, but this works for now.<br>\n",
    "I'll update with some better metrics later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "summ = 0\n",
    "for i,j in zip(y_predict, Y_test):\n",
    "    a = np.where(i==1.0)[0][0]\n",
    "    b = np.where(j==1.0)[0][0]\n",
    "#     print(a, b)\n",
    "    summ += (a==b)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95625"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summ/320"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I still have a few ideas to make the accuracy even better, both in terms of modifing the architecture and augmenting the dataset.\n",
    "That's what I'll be working on for the next few iterations of this "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
